---
title: "`tectr`: Paket zur Konstruktion eines Indikatorsystems"
author: "Samuel Lippl"
date: "`r Sys.Date()`"
output: html_document
vignette: >
  %\VignetteIndexEntry{Expose}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Einführung: Fragilität von Staaten

Die *Fragilität* von Staaten, zu der ich im Rahmen des sogenannten SPIDER^[für eine Projektbeschreibung, die am Montag (22.01.2018) ein kleines Update erhält, aber zum momentanen Zeitpunkt leider veraltet ist, siehe <a href = "https://www.unibw.de/bw/institute/unternehmensfuehrung/uf_professuren/sargl/sa_forschung/spider"> die Projektbeschreibung (Stand 2015)</a>] einen Beitrag als studentische Hilfskraft bei <a href="https://www.unibw.de/bw/institute/unternehmensfuehrung/uf_professuren/sargl/">Prof. Manfred Sargl</a> leiste, beschäftigt sich mit der strukturellen Analyse von Staaten im Hinblick auf die Krisenanfälligkeit. Da Krisen sich oft in Bürgerkriegen und anderen bewaffneten Konflikten manifestieren, ist dise Arbeit eng verwandt mit der Konfliktforschung.  
Diese Disziplin arbeitet zum großen Teil mit einfachen Modellen und wenigen prädiktiven Variablen. Diese Methodik wird sogar empfohlen [Schrodt] und leitet sich vom Ziel der Konfliktforschung ab. Vorhersagen werden verwendet, um den Effekt einzelner Variablen abzuschätzen [Hegre et al. 2017]. Je sparsamer das Modell^[sofern kritische Variablen wie Population oder BIP pro Kopf berücksichtigt werden.], desto einfacher dessen Analyse.  
Es ist nicht sonderlich kompliziert, in diesem Rahmen eine Arbeitsweise zu etablieren, die Overfitting vorbeugt und eine schrittweise Verbesserung des Modells erlaubt^[ein Beispiel hierfür wäre Box's Loop [Colaresi]].  
Während die Konfliktforschung den Aufbau einer Theorie verfolgt, wie Konflikte entstehen und eskalieren ist, dient SPIDER einer konkreten Anwendung. Das hat einige Konsequenzen. Auf einige davon sind mein Kommilitone Fabian Obster und ich bei der Vorbereitung einer vorläufigen Präsentation gestoßen. In deren Rahmen ist eine <a href="https://samue.shinyapps.io/Indikatoruebersicht/">kleine App</a> zur Übersicht der bis dahin eingelesenen Variablen entstanden, aus deren Komplikationen wir bereits viel gelernt haben.
<p>
* die Vorhersagen sollen so viele Informationen wie möglich nutzen. Wir haben momentan 71 Indikatoren gegeben, die wir in unser System integrieren wollen.
* der Einfluss einzelner Variablen soll verständlich sein. Dies ist ein entscheidender Teil möglicher Milderungsmaßnahmen.
* darüber hinaus sollte Overfitting in dieser Evaluation vermieden werden.
* es soll insbesondere klarwerden, wie eine Vorhersage entstanden ist. Ein Modell mag noch so gut sein; wenn es nicht einfach zu verstehen ist, wird es nicht zur Anwendung kommen. Insbesondere muss das Programm einfach zu bedienen sein, auch ohne R-Kenntnisse. (Das war der Hintergedanke der App.)
* die Performance des Vorhersageprogramms muss eine flüssige Anwendung erlauben. (Das ist ein Problem der App im momentanen Zustand. Weil die Visualisierung der Indikatoren auf Landkarten mit großen Mengen an räumlichen Daten arbeitet, muss man bei jeder Untersuchung darauf warten, dass die Graphiken neu laden.)
* das Programm sollte sich automatisch aktualisieren. Es wäre unpraktisch, wenn sich jemand manuell um Aktualisierung kümmern müsste, sobald sich eine Datenbank, die als Quelle dient aktualisiert. (Das ist in der App noch nicht implementiert.)
* das Programm sollte (für Leute mit R-Kenntnissen) selbstverständlich gut zu maintainen sein.
</p>

Gerade der letzte Punkt ist in unserer App hochproblematisch. Natürlich war die perspektivische Anwendung dieser speziellen App viel schmaler und es war mehr oder weniger als Probedurchlauf angedacht, ob das skizzierte Programm wohlwollend aufgenommen würde. Die Reaktionen waren tatsächlich positiv.^[Speziell haben wir festgestellt, dass kleine Flexibilitäten wie die Änderung der Farbkodierung von großem Nutzen wären.] In diesem Sinne haben wir uns also im letzten Monat an die Implementierung des tatsächlichen Programms gesetzt. Wir waren uns schnell einig, dass es sinnvoll ist, für das Indikatorsystem ein Paket zu schreiben, das nicht auf der konkreten Anwendung beruht.^[Auf die Vorteile dieser Abstraktion gehe ich hier nicht weiter ein, das habe ich schließlich zum Teil bei Ihnen gelernt.]  
Diesem Paket haben wir den Arbeitstitel `tectr` gegeben. Während sich die Implementierung als am konkreten Beispiel von SPIDER messen lassen soll, sollte sie genauso geeignet für andere Indikatorsysteme sein. Ich hoffe, ich habe mit den genaueren Informationen über meinen Hintergrund den nun folgenden, eher abstrakten Ideen einen möglichen Kontext verschaffen können.

# "Disclaimer"

## Arbeitsteilung

Bisher ist nicht wirklich klargeworden, wie sich meine Aufgaben und die meines Kollegen unterscheiden, weshalb ich dies in einem kurzen Abschnitt klarstellen will.  
Während wir gemeinsam an SPIDER arbeiten, habe ich bislang die Implementierung in R bis auf wenige Ausnahmen übernommen, weil ich mehr Erfahrung darin habe. Das Konzept, das ich im Folgenden darlege, habe ich mir überlegt und anschließend in Diskussion mit Fabian verfeinert. 
Insbesondere stammt der gesamte bisherige Code in `tectr` von mir und in der nächsten Zeit wird diese Arbeitsteilung sich auch so fortsetzen. Sollte es nötig sein, diese Arbeit bis zum Ende der Bachelorarbeit gänzlich aufzuspalten, wäre dies von unser beider Seite kein Problem. In jedem Fall wäre durch die Versionskontrolle mit Git und Github eine transparente Einsicht möglich, wer was geschrieben hat.

## Einschreibung

Ich bin eigentlich Student der Mathematik und habe im letzten Wintersemester einen Bachelor in Statistik als Doppelstudium begonnen. In Rücksprach mit Prof. Augustin und Prof. Schmid sollte ich dem Doppelstudium eine einjährige Testphase geben, um zu sehen, ob ich damit klarkomme. In diesem Sinne werde ich mich erst zum nächstmöglichen Zeitpunkt, also Ende Februar, offiziell einschreiben. Hierfür habe ich nochmal Rücksprache mit Prof. Schmid gehalten. Es wäre erst danach möglich, mit der Bachelorarbeit zu beginnen.

# Das Ziel von `tectr`

Im allgemeinsten Sinne ist `tectr` als Package in R angelegt, das ein Interface für statistische Analysen bietet. Konkreter gibt es drei (miteinander verknüpfte) Ziele, für die ich `tectr` insbesondere ausgelegt habe.
<p>
* *Interface* zwischen dem Wissenschaftler^[der Einfachheit halber nenne ich diesen von nun an nur noch Wissenschaftler] der Anwendungsdisziplin (möglicherweise ohne R-Kenntnisse) und dem Statistiker.
    + `tectr` soll den Wissenschaftler dabei unterstützen, seine Vorstellung statistisch zu operationalisieren. Zum Beispiel war am Anfang meiner Anstellung, die konkreteste Definition von **Fragilität** durch **Krisenanfälligkeit** gegeben und wir haben im Folgenden festgelegt, dies durch eine Vorhersage eines Maßes für Krise (z. B. Tote in bewaffnetem Konflikt) zu operationalisieren. Im <a href="http://www.oecd.org/dac/conflict-fragility-resilience/states-of-fragility-2016.htm">Bericht der OECD zu **States of Fragility** von 2016</a> wurde dieser statistische Bezug auf Krise bei der Herleitung ihres Maßes für Fragilität nicht berücksichtigt.
    + Darüber hinaus soll `tectr` dem Statistiker eine konkrete Möglichkeit geben, die Erkenntnisse und deren Verbindung zum fachlichen Hintergrund dem Wissenschaftler zu vermitteln. Das ist auch nützlich, wenn der Wissenschaftler die statistische Analyse selbst vornimmt und sie Kollegen vermitteln will.
* Umgang mit über die Zeit hinweg sich erweiternden Datenbanken (die z. B. jedes Jahr neue Messungen bekommen).
    + Automatische Aktualisierung der Quellen und mögliche Anpassung des Systems
    + Es sollte insbesondere transparent gemacht werden, auf welche Daten sich eine Vorhersage beruft
    + Daten zu staatlichen Konflikten lassen sich durch Experimente nicht gewinnen. Während in vielen Kontexten Hypothesentests und Evaluationen eines Modells einen konkreten Rahmen besitzen, wie sie anzuwenden sind, kann das im Falle solcher Daten schnell kompliziert werden. `tectr` soll hierfür Unterstützung bieten.

</p>

# Implementierung

Im Herzen von `tectr` steckt ein *Data Frame* mit einer Observation pro Zeile und einer Variable in jeder Spalte. Dieses Data Frame füllt sich dynamisch und von allein, sobald die externen Quellen und die Beziehungen zwischen den Variablen festgelegt worden sind. Dies ist der Zweck der Klassen `Compound`, `Operator` und `Structure`.  
Am Anfang der Arbeit steht jedoch nicht das Data Frame, sondern ein Wissenschaftler der Anwendungsdisziplin. Im Zentrum seiner Arbeit steht der **Content**, der die statistische Arbeit formen soll.
```{r}
devtools::load_all()
inds <- Content(c("Fragilität", "Kapazität", "Autorität"), 
                c("Krisenanfälligkeit des Staates", 
                  "Wie gut versorgt der Staat die Bürger mit Gütern?", 
                  "Wie gut wahrt der Staat sein Gewaltmonopol?"))
inds
```

Am Ende der Beschreibung dieses Contents sollten alle Inhalte von einem Statistiker operationalisiert werden können. Es müssen also entweder externe Quellen vorliegen oder eine Anleitung, wie sich der Content aus anderen Contents zusammensetzt. Hierfür sind **C_Relations**, spezielle **C**ontents, entscheidend.
```{r}
rels <- C_Relation(c("Teil von"))
rels
```

Contents und C_Relations lassen sich zu **Structures** zusammensetzen, mit denen die Beziehung zwischen den Contents klar wird (die Visualisierung ist noch sehr archaisch):

```{r}
frag <- Structure(variables = inds, 
                  relations = rels,
                  edges = data.frame(from = c("Kapazität", "Autorität"), 
                                     to = c("Fragilität", "Fragilität"), 
                                     name = c("Teil von", "Teil von")))
frag
visualize_struct(frag)
```

Weil aber diese Contents nicht messbar sind, muss der Wissenschaftler genauer angeben, was er wünscht. Um mehrere Arbeitsschritte zu vereinfachen und zu kürzen:

```{r}
library(dplyr)
frag <- frag %>% 
  extend_variables(Content(c("Krise", "Kriegstote"), c("In einem Staat gibt es große Probleme.", "UCDP-Messung"))) %>% 
  extend_relations(C_Relation(c("misst", "sagt vorher"))) %>% 
  extend_edges(data.frame(from = c("Kriegstote", "Fragilität"), 
                          to = c("Krise", "Krise"), 
                          name = c("misst", "sagt vorher")))

frag
visualize_struct(frag)
```

In weiteren Arbeitsschritten müssten Autorität und Kapazität näher charakterisiert werden, doch in diesem Sinne lässt sich eine anständige Beschreibung des inhaltlichen Modells finden, die die Operationalisierung leicht macht.  
Natürlich ist diese Funktion allein nicht sonderlich nützlich; für dieses Vorgehen braucht man wohl keine Anleitung und noch weniger R. Der Nutzen dieser Strukturen liegt im nächsten Arbeitsschritt, den ich noch nicht implementiert habe.  
Für jeden Content wird nun ein **Operator** definiert, für jede C_Relation eine **Relation**.^[Das heißt, jeder Content hat mindestens einen Operator. Er kann ebenso mehreren Operators zugeteilt sein wie mehrere Operators sich auf einen Content beziehen können.]  
Grob gesagt sind die Operators die Variablen unseres Data Frames und die Relations die Regeln wie sich die Operators herleiten.  
Um dies rigoros auszudrücken, differenzieren wir zwischen `Internal_Ops`,  `External_Ops` und `Relations` als Subklassen von `Operators`:
<p>
* **External_Ops** haben eine externe Quelle, z. B. eine Datenbank im Internet oder ein lokales csv-File. External_Ops haben also einen Namen und ein Filename. Sie füllen sich durch eine `read(file)`-Funktion. Hierbei lässt sich eine Standard `read.external_op(file)`-Funktion einrichten. Darüber hinaus lässt sich der Quellentyp als Subklasse einrichten, im Sinne von `External_Op("Lebenserwartung", "C:/Lebenserwartung.csv", subclass = "worldbank")`. Mit `read.worldbank(file)` lässt sich nun dieses File mit einer eigenen Methode einlesen. Natürlich kann man hierbei auch von Subsubklassen etc. profitieren. Dies kann - wie wir selbst bereits festgestellt haben - sehr hilfreich sein, wenn man mit unterschiedlichen Quellen umgehen muss.
* **Internal_Ops** 
</p>
