@article{Schrodt2014,
abstract = {A combination of technological change, methodological drift and a certain degree of intellectual sloth, particularly with respect to philosophy of science, has allowed contemporary quantitative political analysis to accumulate a series of dysfunctional habits that have rendered much of contemporary research more or less meaningless. I identify these ‘seven deadly sins' as: Garbage can models that ignore the effects of collinearity; Pre-scientific explanation in the absence of prediction; Excessive reanalysis of a small number of datasets; Using complex methods without under- standing the underlying assumptions; Interpreting frequentist statistics as if they were Bayesian; A linear statistical monoculture that fails to consider alternative structures; Confusing statistical controls and experimental controls. The answer to these problems is not to abandon quantitative approaches, but rather engage in solid, thoughtful, orig- inal work driven by an appreciation of both theory and data. The article closes with suggestions for changes in current practice that might serve to ameliorate some of these problems. Keywords},
author = {Schrodt, Philip A.},
doi = {10.1177/0022343313499597},
file = {:C$\backslash$:/Users/samue/OneDrive/Studium/Wirtschaftswissenschaften/SoF States of fragility/SoF4.23 Seven deadly sins.pdf:pdf},
issn = {00223433},
journal = {Journal of Peace Research},
keywords = {Bayesian approaches,political analysis,quantitative methods},
number = {2},
pages = {287--300},
title = {{Seven deadly sins of contemporary quantitative political analysis}},
volume = {51},
year = {2014}
}
@article{Hegre2017,
abstract = {Prediction and forecasting have now fully reached peace and conflict research. We define forecasting as predictions about unrealized outcomes given model estimates from realized data, and predictions more generally as the assignment of probability distributions to realized or unrealized outcomes. Increasingly, scholars present within- and out-of-sample prediction results in their publications and sometimes even forecasts for unrealized, future outcomes. The articles in this special issue demonstrate the ability of current approaches to forecast events of interest and contributes to the formulation of best practices for forecasting within peace research. We highlight the role of forecasting for theory evaluation and as a bridge between academics and policymakers, summarize the contributions in the special issue, and provide some thoughts on how research on forecasting in peace research should proceed. We suggest some best practices, noting the importance of theory development, interpretability of models, replicability of results, and data collection.},
author = {Hegre, H{\aa}vard and Metternich, Nils W. and Nyg{\aa}rd, H{\aa}vard Mokleiv and Wucherpfennig, Julian},
doi = {10.1177/0022343317691330},
file = {:C$\backslash$:/Users/samue/OneDrive/Studium/Wirtschaftswissenschaften/SoF States of fragility/SoF4.17 Review 2017.pdf:pdf},
issn = {14603578},
journal = {Journal of Peace Research},
keywords = {Forecasting,Out of sample evaluation,Peace research,Prediction,Theory testing},
number = {2},
pages = {113--124},
title = {{Introduction: Forecasting in peace research}},
volume = {54},
year = {2017}
}
@article{Colaresi2017,
abstract = {Increasingly, scholars interested in understanding conflict processes have turned to evaluating out-of-sample forecasts to judge and compare the usefulness of their models. Research in this vein has made significant progress in identifying and avoiding the problem of overfitting sample data. Yet there has been less research providing strategies and tools to practically improve the out-of-sample performance of existing models and connect forecasting improvement to the goal of theory development in conflict studies. In this article, we fill this void by building on lessons from machine learning research. We highlight a set of iterative tasks, which David Blei terms ‘Box's loop', that can be summarized as build, compute, critique, and think. While the initial steps of Box's loop will be familiar to researchers, the underutilized process of model criticism allows researchers to iteratively learn more useful representations of the data generation process from the discrepancies between the trained model and held-out data. To benefit from iterative model criticism, we advise researchers not only to split their available data into separate training and test sets, but also sample from their training data to allow for iterative model development, as is common in machine learning applications. Since practical tools for model criticism in particular are underdeveloped, we also provide software for new visualizations that build upon already existing tools. We use models of civil war onset to provide an illustration of how our machine learning-inspired research design can simultaneously improve out-of-sample forecasting performance and identify useful theoretical contributions. We believe these research strategies can complement existing designs to accelerate innovations across conflict processes.},
author = {Colaresi, Michael and Mahmood, Zuhaib},
doi = {10.1177/0022343316682065},
file = {:C$\backslash$:/Users/samue/OneDrive/Studium/Wirtschaftswissenschaften/SoF States of fragility/SoF4.5 Box's loop (Colaresi {\&} Mahmood 2017).pdf:pdf},
issn = {14603578},
journal = {Journal of Peace Research},
keywords = {Civil war,Forecasting,Machine learning,Methodology,Visualization},
number = {2},
pages = {193--214},
title = {{Do the robot: Lessons from machine learning to improve conflict forecasting}},
volume = {54},
year = {2017}
}
