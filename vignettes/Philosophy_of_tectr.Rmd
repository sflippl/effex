---
title: "Philosophy of tectr"
author: "Samuel Lippl"
date: "`r Sys.Date()`"
bibliography: "tectr.bib"
bibliostyle: "apalike"
link-citations: true
output: 
  rmarkdown::html_vignette: 
    fig_caption: yes
    number_sections: yes
vignette: >
  %\VignetteIndexEntry{Philosophy of tectr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction: The Three Goals of tectr

The discipline of statistics is made both more challenging and more interesting by the synthesis of many different perspectives on the identical problem: the *expert*, who does not have the required skills but is interested in a statistical analysis and knows about the discipline-specific context, which is crucial for a suitable analysis of some data within this discipline. However, it needs to be complemented by a *statistical* perspective: in order to analyze data, one needs to ask appropriate questions. Finally, there is an *implementation* perspective, as well: it does not suffice to know that a linear model (or a principal component analysis or a neural network) is an appropriate approach; it is also necessary to implement this approach with the suitable functions and packages.

`tectr` attempts to simplify the relationship between these perspectives by supporting communication where communication is suitable and separation where separation is necessary. I will formulate my approach using three essential aims which `tectr` will support:

* make the data talk,
* separate implementation from analysis, 
* code decisions where you make decisions.

I will introduce `tectr` with respect to these goals.

# Make The Data Talk

## General Principle

The first principle refers to the fact that without both the expert's and the statistician's perspective, the data does not make a lot of sense.

For instance, if we are interested in the connection between genetic expression and a certain behaviour, the expert -- who might be a biologist -- knows, more generally, why genetic expression influences behaviour at all and, more specifically, the functions of a specific gene that is analyzed. I will call this *semantic knowledge*. Without semantic knowledge, gene expression data is just a sparse matrix of 0s and 1s.

On the other hand, the statistician provides the appropriate methods to analyze the connection between expression data and behaviour. While he may not possess the necessary semantic knowledge to interpret them, he knows what conclusions may be drawn from the results of the statistical analysis. I will call this knowledge complex *statistical knowledge*. For a successful statistical analysis, the expert needs to convey the necessary semantic knowledge to the statistician and the statistician, in turn, needs to convey the necessary data-specific knowledge, both of the methods and the results.

The following diagram visualizes the task:

```{r makeittalk, out.width = "50%", echo = FALSE}
knitr::include_graphics(path = "figures/1_makeittalk.png")
```

The grey arrows represent tasks that need to be done manually. `tectr` takes on the blue arrows. Note that the diagram is simplified; the expert might profit from a centralized summary of the relevant information, as well. Furthermore, interested outsiders might profit from both statistical and semantic information -- for instance in the context of a publication. A more complete diagram of the mechanism is supplied below:

```{r makeittalk_extended, out.width = "70%", echo = FALSE}
knitr::include_graphics(path = "figures/1_makeittalk_extended.png")
```

## Knowledge Management

While a philosophical discussion of the concept of knowledge is certainly not necessary, a short elaboration on useful concepts with respect to statistical knowledge is sensible as `tectr` may be considered from the perspective of **knowledge management**. I will evade the messy problem of defining knowledge by emphasizing that only persons may possess knowledge and referring the interested reader to the review by @sep-epistemology.

A first important distinction must be made between knowledge and information. Information is "a flow of messages, while knowledge is created and organized by the very flow of information, anchored on the commitment and beliefs of its holder" [@Nonaka1994]. More concisely, knowledge is "personalized information" [@Alavi2001].

What does it mean to make the data talk, then? The expert and the statistician have different perspectives on the data. The expert has a contextual view of the variables and the connections between the variables. Such a view is crucial to determine, justify and understand a suitable model. The expert therefore needs to convert his **knowledge** about the discipline into **information** about the variables, their connections and concepts that are related to them.

On the other hand, the statistician knows about the models and other statistical concepts. He needs to present this knowledge in a way such that the expert can understand their application and interpret the results. [^1] Although dealing with another discipline, the concepts and tasks with respect to `tectr` remain the same. (Note that this does not refer to data statistics etc. but the meaning of e. g. a linear model.)

[^1]: Of course, most experts have an understanding of simple statistical concepts like a mean. On the other hand, there might be some contextual information on these statistics (e. g. that the mean is vulnerable to outliers) that is relevant in certain applications but not known to the expert.

As mentioned before, both perspectives contribute to an understanding of the data. Managing information from these perspectives corresponds to the task of knowledge-based management system (KBMS, cf. @Alavi2001). These systems have wide applications and may be nearly arbitrarily complex. If we google "Population of Germany", Google returns a graph of the population. On the other hand, an IDE of a programming language (e. g. RStudio) assists in software design. Both programs can be seen as KBMSs. As `tectr` can be formulated as a KBMS, we may build upon the research within the discipline: @Jarke1985 define four general needs of KBMS customers:

* knowledge representation lanuages that express the structure of the given application effectively, 
* knowledge organization tools that allow for the safe and efficient handling of large amounts of complex knowledge structures, 
* methodologies and environments through which the customer can create, maintain and query knowledge bases efficiently and effectively, 
* support for the re-use of existing hardware and software facilities.

The last point is especially important with respect to `tectr`. A package in R should not reinvent the wheel and impose minimal training costs on the user. It should thus conform to the conventions of the R programming language and fit within some larger framework. `tectr` is therefore not suited for a complex management of semantic knowledge.

## Implementation of Semantic Knowledge Management

The most prominent implementation of some semantic knowledge management system within R is the R documentation that consists of articles. Usually, these articles cover specific objects in R like functions, datasets or packages, but an article can be supplied on an arbitrary topic. I therefore propose, as the first feature of `tectr`, a semantic knowledge management system that is compatible with the R documentation. It should be possible to access the articles on the semantic knowledge that is relevant with respect to the specific statistical analysis via the R documentation (e. g. `?` or `help`). Additionally, I propose some useful functionality:

* There should be some function `doc(topic, format)` that returns the documentation on a certain topic in a specific format, e. g. .Rmd or .pdf. A user-definable format like `plot_info` would allow its integration within an opject like a plot, e. g. in the form of a small box with a short variable definition.

* There should be a feature that allows the expert to add semantic knowledge without knowing R, for instance, using a Shiny app where it is possible to either write or upload articles.

* The extensiveness of the resulting articles should be adaptable (see the upper example of `plot_info`). A possible implementation might be given by different fields, like they are implemented in the R documentation.

## Data-Analytic Knowledge Management

Based on semantic knowledge management, we may consider the core of `tectr`: the data-analytic knowledge management. Information is sometimes seen as "processed data" [@Alavi2001], a concept which applies to the data-analytic domain. With the semantic knowledge, we may identify relevant models and statistics, apply them und analyze the results. This allows us to make the data talk.

The re-use maxim, which restricted the complexity of semantic knowledge management, implies that data-analytic knowledge management must encompass many different forms of data-specific knowledge: statistics like a mode or a median, visualizations like a scatterplot or a heatmap, tables that show a subset of the data etc. After all, R is a statistical programming language. The remaining two essential aims that are presented in this vignette therefore mostly refer to data-analytic knowledge management.

# Separate Implementation From Analysis

Of course, R already provides an expert system for data-analytic knowledge management. However, this compares to the aim of tectr like a block of marble to a statue or a library of all possible books to a proof of the Riemann hypothesis: the data exists but this does not mean that any information can be extracted. Most importantly, the expert should not be expected to know R. This means that the expert either needs the statistician for every single analysis or a solution that does not depend on knowledge of R needs to be found. The latter consideration has sparked packages like `shiny` [@R-shiny] that support interactive data analysis and serves as evidence for the aim to separate implementation from analysis.

The form in which `tectr` approaches this aim as well as the purpose of the aim itself, however, can be motivated without regard to anyone's skills in R. Let us therefore consider a statistician who explores a certain dataset. For instance, he may want to explore the univariate distribution of the variables using a density plot of continuous variables and a barplot of the five most frequent categories of discrete variables. Furthermore, he may wish to use a colour scale to set apart observations of different categories of a certain variable. Such an attempt requires a rather long command -- especially in the case of further complications; e. g. an inconsistent colour scale -- even though he has figured out what he wants within a few variables. Implementational details may therefore occupy a considerable amount of space within his workspace or his attention, at a time when he wishes to analyze the distribution of the data. A suitable solution is defining, for instance, two functions `explor_bar` and `explor_dens`, or even one function `explor` that decides whether to employ `explor_bar` or `explor_dens`. The statistician therefore separates implementation from analysis. From now on, he only needs to decide on the variable and then type `explor(df$var)`.

`tectr` wishes to simplify this approach. While in this simple example, the functions are not very complicated some exploratory analysis requires more considerable preparation so that an efficient implementation both of this preparation as well as of the connection to the final descriptive and inductive analysis is useful. Furthermore, while an analysis that is separate from implementation might still require coding, some statisticians might prefer a less code-heavy, more interactive way of exploratory analysis which tectr supports without considerable effort, as well. This might be implemented, for instance, by a function `explore`.

The following diagrams visualize the described problem. At the left, three different kinds of analysis are visualized by the differently coloured plots. The statistician always needs to be concerned with the implementational details. On the right, every analysis needs to be initiated once with tectr. Afterwards, the statistician needs not consider any implementational details.

```{r separate, out.width = "40%", echo = FALSE}
knitr::include_graphics(path = "figures/1_separate_a1.png")
knitr::include_graphics(path = "figures/1_separate_a2.png")
```

The same principle applies to reading in data, for instance. Often, there are several different files using the same format where some data tidying and/or cleaning needs to be done. An implementation-separate approach is much cleaner:

```{r eval = FALSE}
df <- get_variable("worldbank-gdp", "worldbank-population")
```

`tectr` thus aims to support abstraction from implementation, by providing both a consistent interface and useful support functions. In particular, this supports communication both between different statisticians and between statisticians and experts:

* Once a statistician has read data from a specific source (e. g. the Worldbank) into R, it becomes very easy for any other user to use and explore this data:
```{r eval = FALSE}
library(worldbank)
df <- get_variable("worldbank-gdp")
ggdistribution(df$worldbank__gdp) # I will explain these implementational details
                                  # in another vignette
```

* The consistent interface allows the effortless construction of a shiny app to explore a dataset by specifying the interesting variables and the preferred visualizations without needing to detail on the particular plots or details like a consistent colour scale. For instance, the function `ggdistribution` may visualize the distribution of the GDP as a histogram with log-scale and the distribution of countries in the Worldbank dataset over the different continents by a barplot.

The following diagram visualizes this generalization:

```{r separate_b, echo = FALSE, out.width = "70%"}
knitr::include_graphics(path = "figures/1_separate_b.png")
```

These considerations are related to the first aim of making the data talk; the less the analyst needs to be concerned with implementational details the more time he can spend with getting to know the data.

# Code Decisions Where You Make Decisions

The third aim may be seen as a generalization of the second. A successful implementation of this principle can be seen in the package `roxygen2` [@R-roxygen2], which solves the awkward separation of documentation and documented function. In the same way, it is useful to specify all details that concern some variable, data source or another concept at one place. For instance, it is useful to document the semantic meaning of a variable at the same place as its format (e. g. `$`), whether to include it in a certain model and how to visualize its distribution. Even if these decisions are not made at one time, it is useful to have one place where all these decisions are specified. Furthermore, such an approach may profit from an extensive class hierarchy such that most of these decisions only need to be made at one place. The following pseudocode demonstrates the envisioned syntax.

```{r eval = FALSE}
gdp <- new_variable(
  class = "continuous"
  source = "Worldbank", 
  description_path = "desc/wb/gdp.Rmd", 
  suffix = "$"
) %>% 
  add_scale(x = "log10", colour = "Blues") %>% 
  add_model(lm_conflict) %>% 
  add_model(mboost_conflict, effect = c("bbs(., center = TRUE, df = 1)", 
                                        "bols(.)"))
```

Note the two different ways to change a variable, depending on personal preferences and structure of the project / package. The `suffix` argument changes the legend / axis of a plot as well as a printed table. The `class` argument lets gdp inherit from a certain class of variables. For instance, `continuous` might specify the design of canonical plots. The function `add_models` changes the formula of a certain model. Depending on the method, additional parameters may be provided.

Note that this structure further supports the second aim as these decisions are made once and do not need to be specified afterwards. For instance, the class `visualize` might specify a geometry `geom_standard`. Depending on the particular variables, the class "continuous" might for example specify that a call

```{r eval = FALSE}
ggplot(df, aes(x = gdp)) + geom_standard()
```

might depict a density plot (with a `log10`-scale, of course). On the other hand, we could change `gdp`:

```{r eval = FALSE}
gdp <- gdp %>% change_geom("standard", x = "hist")
```

Now, the same command would yield a histogram. Of course, several variables could also be visualized at the same time. For instance, we could specify that 

```{r eval = FALSE}
ggplot(df, aes(x = gdp, y = population, colour = continent)) + 
  geom_standard()
```

shows a scatterplot. [^2] Furthermore, it may include semantic descriptions of the three variables at appropriate positions.

[^2]: Note that I have not yet found a suitable syntax and the pseudocode implementation of these examples should not be taken too seriously.

# Summary

As an example, I will describe possible approach to a statistical analysis: suppose that Fred (the expert) is interested in a statistical analysis of the relationship between the export of grapes and different socioeconomic variables. He employs Gina as a statistical analyst and specifies relevant variable information, for instance:

> GDP per Capita
>
> *Description* The GDP, normalized by population.
>
> *Comment* Countries with a high GDP have a lower need for grapes and therefore export them.

Gina receives these specifications and explores the specific data. Where necessary, she will define the function `get_variable` herself. She can then supply these specifications to the community in the form of a package. Such a package may already exist for Worldbank data:

```{r eval = FALSE}
library(worldbank)
df <- get_variable("worldbank-gdp") %>% 
  get_variable("somesource-land_covered_in_grapes", .) # 2nd argument is already existing data
```

Afterwards, she meets up with Fred and in their discussion they specify two different models, a conservative linear model of certain variables without interactions (`conslm`) and a linear model with interactions (`interlm`). The variable information is now extended:

> GDP per Capita
>
> *Description* The GDP, normalized by population.
>
> *Comment* Countries with a high GDP have a lower need for grapes and therefore export them.
>
> *Models* `conslm`, `lm_interactions("interlm", "somesource-land_covered_in_grapes")`

Now, Gina can implement the model:

```{r eval = FALSE}
conslm <- tectr_lm("conslm")
interlm <- tectr_lm("interlm")
conslm_pred <- tectr_predict(conslm)
# ...
df <- get_variable("worldbank-gdp") %>% 
  get_variable("somesource-land_covered_in_grapes", .) %>% 
  # ...
  get_variable(c("conslm", "interlm"), .) %>% 
  get_variable(c("conslm_pred", "interlm_pred"), .)
```

`df` can then be supplied to some interactive exploration analysis. Sensible tools to investigate effects, coefficients etc. must only be supplied once and can be shared afterwards. This principle makes such an analysis both efficient and effective. Furthermore, if one is interested, for instance, in a visualization of some variable with a map, this can be supplied effectively by adding an identification of the Worldbank country and some coordinate polygon and then supplying:

```{r eval = FALSE}
plot <- df %>% 
  select_view(view = worldbank, 
              worldbank__gdp, wb_coords__geometry, worldbank__year) %>% 
  filter(worldbank__year == 2015) %>% 
  ggplot(aes(fill = worldbank__gdp, geometry = wb_coords_geometry)) + 
  geom_map()
```


# References
